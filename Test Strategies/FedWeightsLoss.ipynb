{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22428,"status":"ok","timestamp":1717355347338,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"6-fd0vkXRjh-","outputId":"6c0154c2-0e5d-470e-cbd7-e380661f2620"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: fedartml in /usr/local/lib/python3.10/dist-packages (0.1.33)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fedartml) (1.25.2)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from fedartml) (7.7.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fedartml) (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fedartml) (2.0.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fedartml) (1.11.4)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from fedartml) (2.15.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from fedartml) (2.15.0)\n","Requirement already satisfied: ipykernel\u003e=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (0.2.0)\n","Requirement already satisfied: traitlets\u003e=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (3.6.6)\n","Requirement already satisfied: ipython\u003e=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (7.34.0)\n","Requirement already satisfied: jupyterlab-widgets\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets-\u003efedartml) (3.0.10)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (1.2.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (4.51.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (1.4.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (24.0)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (3.1.2)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003efedartml) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003efedartml) (2023.4)\n","Requirement already satisfied: tzdata\u003e=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003efedartml) (2024.1)\n","Requirement already satisfied: absl-py\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (1.4.0)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (1.6.3)\n","Requirement already satisfied: flatbuffers\u003e=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (0.5.4)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (0.2.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (3.9.0)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (0.2.0)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u003c5.0.0dev,\u003e=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (4.25.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (67.7.2)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (1.16.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (2.4.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (4.11.0)\n","Requirement already satisfied: wrapt\u003c1.15,\u003e=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (0.37.0)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (1.64.0)\n","Requirement already satisfied: tensorboard\u003c2.16,\u003e=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator\u003c2.16,\u003e=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-\u003efedartml) (2.15.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow-\u003efedartml) (0.43.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel\u003e=4.5.1-\u003eipywidgets-\u003efedartml) (6.1.12)\n","Requirement already satisfied: tornado\u003e=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel\u003e=4.5.1-\u003eipywidgets-\u003efedartml) (6.3.3)\n","Requirement already satisfied: jedi\u003e=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,\u003c3.1.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.1.7)\n","Requirement already satisfied: pexpect\u003e4.3 in /usr/local/lib/python3.10/dist-packages (from ipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (4.9.0)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c2,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (1.2.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (3.6)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (3.0.3)\n","Requirement already satisfied: notebook\u003e=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (6.5.5)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (5.3.3)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (0.4.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (1.3.1)\n","Requirement already satisfied: parso\u003c0.9.0,\u003e=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi\u003e=0.16-\u003eipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.8.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (3.1.4)\n","Requirement already satisfied: pyzmq\u003c25,\u003e=17 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (23.1.0)\n","Requirement already satisfied: jupyter-core\u003e=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (5.7.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (5.10.4)\n","Requirement already satisfied: nbconvert\u003e=5 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (6.5.4)\n","Requirement already satisfied: nest-asyncio\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.6.0)\n","Requirement already satisfied: Send2Trash\u003e=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.8.3)\n","Requirement already satisfied: terminado\u003e=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.20.0)\n","Requirement already satisfied: nbclassic\u003e=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.0.0)\n","Requirement already satisfied: ptyprocess\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect\u003e4.3-\u003eipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,\u003c3.1.0,\u003e=2.0.0-\u003eipython\u003e=4.0.0-\u003eipywidgets-\u003efedartml) (0.2.13)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (2024.2.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (2.1.5)\n","Requirement already satisfied: platformdirs\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core\u003e=4.6.1-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (4.2.2)\n","Requirement already satisfied: jupyter-server\u003e=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.24.0)\n","Requirement already satisfied: notebook-shim\u003e=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.2.4)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.7.1)\n","Requirement already satisfied: entrypoints\u003e=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.3.0)\n","Requirement already satisfied: mistune\u003c2,\u003e=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.8.4)\n","Requirement already satisfied: nbclient\u003e=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.10.0)\n","Requirement already satisfied: pandocfilters\u003e=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.3.0)\n","Requirement already satisfied: fastjsonschema\u003e=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (2.19.1)\n","Requirement already satisfied: jsonschema\u003e=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (4.19.2)\n","Requirement already satisfied: pyasn1\u003c0.7.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (0.6.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c2,\u003e=0.5-\u003etensorboard\u003c2.16,\u003e=2.15-\u003etensorflow-\u003efedartml) (3.2.2)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (21.2.0)\n","Requirement already satisfied: attrs\u003e=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (2023.12.1)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.35.1)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.18.1)\n","Requirement already satisfied: anyio\u003c4,\u003e=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server\u003e=1.8-\u003enbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server\u003e=1.8-\u003enbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.8.0)\n","Requirement already satisfied: cffi\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings-\u003eargon2-cffi-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.16.0)\n","Requirement already satisfied: soupsieve\u003e1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-\u003enbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-\u003enbconvert\u003e=5-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (0.5.1)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c4,\u003e=3.1.0-\u003ejupyter-server\u003e=1.8-\u003enbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c4,\u003e=3.1.0-\u003ejupyter-server\u003e=1.8-\u003enbclassic\u003e=0.4.7-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (1.2.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi\u003e=1.0.1-\u003eargon2-cffi-bindings-\u003eargon2-cffi-\u003enotebook\u003e=4.4.1-\u003ewidgetsnbextension~=3.6.0-\u003eipywidgets-\u003efedartml) (2.22)\n"]}],"source":["!pip install fedartml\n","!pip install -q flwr[simulation]"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13087,"status":"ok","timestamp":1717355360422,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"1lrOq2zTRsIZ"},"outputs":[],"source":["import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n","\n","from io import BytesIO\n","import requests\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","from tensorflow.keras.optimizers import SGD\n","\n","from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n","enable_tf_gpu_growth()\n","\n","import time\n","import pickle\n","\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","# Garbage Collector - use it like gc.collect()\n","import gc\n","\n","# Custom Callback To Include in Callbacks List At Training Time\n","class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        gc.collect()\n","\n","from fedartml import InteractivePlots, SplitAsFederatedData\n","\n","# Make TensorFlow logs less verbose\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","import flwr as fl\n","import math\n","\n","from typing import List, Tuple, Dict, Optional\n","from flwr.common import Metrics"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1717355360423,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"BPoBNPvRRvnA","outputId":"aa94f8b5-523b-45e2-f4db-6c7ab132a9e9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# Define function to test a model and retrieve classification metrics\n","def test_model(model, X_test, Y_test):\n","    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n","    logits = model.predict(X_test, batch_size=8, verbose=2, callbacks=[GarbageCollectorCallback()])\n","    y_pred = tf.argmax(logits, axis=1)\n","    loss = cce(Y_test, logits).numpy()\n","    acc = accuracy_score(y_pred, Y_test)\n","    pre = precision_score(y_pred, Y_test, average='weighted',zero_division = 0)\n","    rec = recall_score(y_pred, Y_test, average='weighted',zero_division = 0)\n","    f1s = f1_score(y_pred, Y_test, average='weighted',zero_division = 0)\n","\n","    return loss, acc, pre, rec, f1s\n","\n","# Define function to convert from SplitAsFederatedData function output (FedArtML) to Flower (list) format\n","def from_FedArtML_to_Flower_format(clients_dict):\n","  # initialize list that contains clients (features and labels) to extract later from client_fn in Flower\n","  list_x_train = []\n","  list_y_train = []\n","\n","  # Get the name of the clients from the dictionary\n","  client_names = list(clients_dict.keys())\n","\n","  # Iterate over each client\n","  for client in client_names:\n","    # Get data from each client\n","    each_client_train=np.array(clients_dict[client],dtype=object)\n","\n","    # Extract features for each client\n","    feat=[]\n","    x_tra=np.array(each_client_train[:, 0])\n","    for row in x_tra:\n","      feat.append(row)\n","    feat=np.array(feat)\n","\n","    # Extract labels from each client\n","    y_tra=np.array(each_client_train[:, 1])\n","\n","    # Append in list features and labels to extract later from client_fn in Flower\n","    list_x_train.append(feat)\n","    list_y_train.append(y_tra)\n","\n","  return list_x_train, list_y_train\n","\n","def get_model():\n","    \"\"\"Constructs a simple model architecture suitable for MNIST.\"\"\"\n","    model = tf.keras.models.Sequential(\n","        [\n","            tf.keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='relu', input_shape=(32,32,3), padding='same'), #C1\n","            tf.keras.layers.AveragePooling2D(), #S1\n","            tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='valid'), #C2\n","            tf.keras.layers.AveragePooling2D(), #S2\n","            tf.keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='relu', padding='valid'), #C3\n","            tf.keras.layers.Flatten(),\n","            tf.keras.layers.Dense(84, activation='relu'),\n","            tf.keras.layers.Dense(10, activation='softmax')\n","        ]\n","    )\n","    return model\n","\n","class FlowerClient(fl.client.NumPyClient):\n","    def __init__(self, model, x_train, y_train, x_test, y_test, epochs_client) -\u003e None:\n","        self.model = model\n","        self.x_train, self.y_train = x_train, y_train\n","        self.x_test, self.y_test = x_test, y_test\n","        self.epochs_client = epochs_client\n","\n","    def get_parameters(self, config):\n","        return self.model.get_weights()\n","\n","    def fit(self, parameters, config):\n","        self.model.set_weights(parameters)\n","        self.model.compile(optimizer=SGD(learning_rate = config[\"learning_rate\"]), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        curr_loss, curr_acc = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n","        self.model.fit(self.x_train, self.y_train, epochs=1, verbose=0, batch_size = 16, callbacks=[GarbageCollectorCallback()])\n","        after_loss, after_acc = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n","\n","        loss = math.exp(curr_loss - after_loss)\n","        return self.model.get_weights(), len(self.x_train), {'loss': loss}\n","\n","    def evaluate(self, parameters, config):\n","        self.model.set_weights(parameters)\n","        self.model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        loss, acc = self.model.evaluate(self.x_test, self.y_test, verbose=2)\n","        return loss, len(self.x_test), {\"accuracy\": acc}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1717355360423,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"IngM-n--Or2b"},"outputs":[],"source":["from flwr.server.client_manager import ClientManager\n","import threading\n","from abc import ABC, abstractmethod\n","from logging import INFO\n","from typing import Dict, List, Optional\n","import random\n","from flwr.server.client_proxy import ClientProxy\n","from flwr.server.criterion import Criterion\n","\n","class SimpleClientManager(ClientManager):\n","    def __init__(self) -\u003e None:\n","        self.clients: Dict[str, ClientProxy] = {}\n","        self._cv = threading.Condition()\n","        self.seed = 0\n","\n","    def __len__(self) -\u003e int:\n","        return len(self.clients)\n","\n","    def num_available(self) -\u003e int:\n","        return len(self)\n","\n","    def wait_for(self, num_clients: int, timeout: int = 86400) -\u003e bool:\n","        with self._cv:\n","            return self._cv.wait_for(\n","                lambda: len(self.clients) \u003e= num_clients, timeout=timeout\n","            )\n","\n","    def register(self, client: ClientProxy) -\u003e bool:\n","        if client.cid in self.clients:\n","            return False\n","\n","        self.clients[client.cid] = client\n","        with self._cv:\n","            self._cv.notify_all()\n","\n","        return True\n","\n","    def unregister(self, client: ClientProxy) -\u003e None:\n","        if client.cid in self.clients:\n","            del self.clients[client.cid]\n","\n","            with self._cv:\n","                self._cv.notify_all()\n","\n","    def all(self) -\u003e Dict[str, ClientProxy]:\n","        return self.clients\n","\n","    def sample(\n","        self,\n","        num_clients: int,\n","        min_num_clients: Optional[int] = None,\n","        criterion: Optional[Criterion] = None,\n","    ) -\u003e List[ClientProxy]:\n","        # Block until at least num_clients are connected.\n","        random.seed(self.seed)\n","        self.seed += 1\n","        if min_num_clients is None:\n","            min_num_clients = num_clients\n","        self.wait_for(min_num_clients)\n","        # Sample clients which meet the criterion\n","        available_cids = list(self.clients)\n","        if criterion is not None:\n","            available_cids = [\n","                cid for cid in available_cids if criterion.select(self.clients[cid])\n","            ]\n","\n","        sampled_cids = random.sample(available_cids, num_clients)\n","        print(sampled_cids)\n","        return [self.clients[cid] for cid in sampled_cids]"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1717355360423,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"8TLwDOyqr-MZ"},"outputs":[],"source":["from logging import WARNING\n","from typing import Callable, Dict, List, Optional, Tuple, Union\n","from functools import reduce\n","\n","from flwr.common import (\n","    FitRes,\n","    FitIns,\n","    MetricsAggregationFn,\n","    NDArrays,\n","    Parameters,\n","    Scalar,\n","    ndarrays_to_parameters,\n","    parameters_to_ndarrays,\n",")\n","from flwr.common.logger import log\n","from flwr.server.client_manager import ClientManager\n","from flwr.server.client_proxy import ClientProxy\n","from flwr.server.strategy.fedavg import FedAvg\n","\n","class FedWeightsLoss(FedAvg):\n","    def __init__(\n","        self,\n","        *,\n","        fraction_fit: float = 1.0,\n","        fraction_evaluate: float = 1.0,\n","        min_fit_clients: int = 2,\n","        min_evaluate_clients: int = 2,\n","        min_available_clients: int = 2,\n","        evaluate_fn: Optional[\n","            Callable[\n","                [int, NDArrays, Dict[str, Scalar]],\n","                Optional[Tuple[float, Dict[str, Scalar]]],\n","            ]\n","        ] = None,\n","        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n","        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n","        accept_failures: bool = True,\n","        initial_parameters: Optional[Parameters] = None,\n","        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n","        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n","\n","    ) -\u003e None:\n","        super().__init__()\n","        self.fraction_fit = fraction_fit\n","        self.fraction_evaluate = fraction_evaluate\n","        self.min_fit_clients = min_fit_clients\n","        self.min_evaluate_clients = min_evaluate_clients\n","        self.min_available_clients = min_available_clients\n","        self.evaluate_fn = evaluate_fn\n","        self.on_fit_config_fn = on_fit_config_fn\n","        self.on_evaluate_config_fn = on_evaluate_config_fn\n","        self.accept_failures = accept_failures\n","        self.initial_parameters = initial_parameters\n","        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n","        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n","        self.learning_rate = 0.01\n","        self.decay = 0.995\n","\n","    def configure_fit(\n","        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n","    ) -\u003e List[Tuple[ClientProxy, FitIns]]:\n","        config = {\"learning_rate\": self.learning_rate}\n","\n","        if self.on_fit_config_fn is not None:\n","            # Custom fit config function provided\n","            config = self.on_fit_config_fn(server_round)\n","        fit_ins = FitIns(parameters, config)\n","        self.learning_rate*=self.decay\n","        # Sample clients\n","        sample_size, min_num_clients = self.num_fit_clients(\n","            client_manager.num_available()\n","        )\n","        clients = client_manager.sample(\n","            num_clients=sample_size, min_num_clients=min_num_clients\n","        )\n","\n","        # Return client/config pairs\n","        return [(client, fit_ins) for client in clients]\n","\n","    def aggregate_fit(\n","        self,\n","        server_round: int,\n","        results: List[Tuple[ClientProxy, FitRes]],\n","        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n","    ) -\u003e Tuple[Optional[Parameters], Dict[str, Scalar]]:\n","        if not results:\n","            return None, {}\n","        # Do not aggregate if there are failures and failures are not accepted\n","        if not self.accept_failures and failures:\n","            return None, {}\n","\n","        # Convert results\n","        weights_results = [\n","            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples * fit_res.metrics['loss'])\n","            for _, fit_res in results\n","        ]\n","        parameters_aggregated = ndarrays_to_parameters(\n","            aggregate(weights_results)\n","        )\n","\n","        # Aggregate custom metrics if aggregation fn was provided\n","        metrics_aggregated = {}\n","        if self.fit_metrics_aggregation_fn:\n","            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n","            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n","        elif server_round == 1:  # Only log this warning once\n","            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n","\n","        return parameters_aggregated, metrics_aggregated"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1717355360423,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"m4ZBaF_WR0qq"},"outputs":[],"source":["# Define random state for reproducibility\n","random_state = 0\n","\n","# Define colors to use in plots\n","colors = [\"#00cfcc\",\"#e6013b\",\"#007f88\",\"#00cccd\",\"#69e0da\",\"darkblue\",\"#FFFFFF\"]\n","\n","# Define number of local nodes to be used\n","local_nodes_glob = 40\n","\n","# Define percentage of noniid to be used\n","Percent_noniid = 90\n","\n","Alpha = 1"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":37877,"status":"ok","timestamp":1717355398283,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"W4EZoVuhR-fY"},"outputs":[],"source":["from tensorflow.keras.datasets import mnist, cifar10\n","from tensorflow.keras.utils import to_categorical\n","\n","# Define random state for reproducibility\n","random_state = 0\n","\n","(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n","\n","train_images = train_images / 255\n","test_images = test_images / 255\n","train_labels, test_labels = np.concatenate(train_labels), np.concatenate(test_labels)\n","\n","my_federater = SplitAsFederatedData(random_state = random_state)\n","\n","clients_glob_dic, list_ids_sampled_dic, miss_class_per_node, distances = my_federater.create_clients(image_list = train_images, label_list = train_labels,\n","                                                             num_clients = local_nodes_glob, prefix_cli='client', method = \"dirichlet\", alpha = Alpha)\n","\n","clients_glob = clients_glob_dic['with_class_completion']\n","list_ids_sampled = list_ids_sampled_dic['with_class_completion']\n","\n","list_x_train, list_y_train = from_FedArtML_to_Flower_format(clients_dict=clients_glob)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1717355398283,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"},"user_tz":-420},"id":"aL8u7X8wSBgD","outputId":"675277ab-0a3b-4df2-e7af-3aadbeee38e7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# The `evaluate` function will be by Flower called after every round\n","def evaluate_DNN_CL(\n","    server_round: int,\n","    parameters: fl.common.NDArrays,\n","    config: Dict[str, fl.common.Scalar],\n",") -\u003e Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n","    net = get_model()\n","    net.set_weights(parameters) # Update model with the latest parameters\n","    loss, accuracy, precision, recall, f1score  = test_model(net, test_images, test_labels)\n","    return loss, {\"accuracy\": accuracy,\"precision\": precision,\"recall\": recall,\"f1score\": f1score}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3Sle-6bDSDZ_"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=300, no round_timeout\n","INFO:flwr:Starting Flower simulation, config: num_rounds=300, no round_timeout\n","/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = _posixsubprocess.fork_exec(\n","2024-06-02 19:10:03,334\tINFO worker.py:1621 -- Started a local Ray instance.\n","\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'memory': 7951606580.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3975803289.0}\n","INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'memory': 7951606580.0, 'node:__internal_head__': 1.0, 'GPU': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3975803289.0}\n","\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n","INFO:flwr:Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n","\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n","INFO:flwr:No `client_resources` specified. Using minimal resources for clients.\n","\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n","INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n","\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n","INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n","\u001b[92mINFO \u001b[0m:      [INIT]\n","INFO:flwr:[INIT]\n","\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n","INFO:flwr:Using initial global parameters provided by strategy\n","\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n","INFO:flwr:Evaluating initial global parameters\n"]},{"name":"stdout","output_type":"stream","text":["1250/1250 - 5s - 5s/epoch - 4ms/step\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.306954, {'accuracy': 0.1003, 'precision': 0.5036946, 'recall': 0.1003, 'f1score': 0.16343141045744075}\n","INFO:flwr:initial parameters (loss, other metrics): 2.306954, {'accuracy': 0.1003, 'precision': 0.5036946, 'recall': 0.1003, 'f1score': 0.16343141045744075}\n","\u001b[92mINFO \u001b[0m:      \n","INFO:flwr:\n","\u001b[92mINFO \u001b[0m:      [ROUND 1]\n","INFO:flwr:[ROUND 1]\n","\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 8 clients (out of 40)\n","INFO:flwr:configure_fit: strategy sampled 8 clients (out of 40)\n"]},{"name":"stdout","output_type":"stream","text":["['24', '26', '2', '16', '32', '31', '25', '19']\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(ClientAppActor pid=14098)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n","\u001b[2m\u001b[36m(ClientAppActor pid=14098)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n","2024-06-02 19:11:32,337\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff38213e3b44769b9bfdeabb9c01000000 Worker ID: fcfe47f249e22e8c4b394344f55ce2aeef7e96c6dfaada36a2edfb75 Node ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614 Worker IP address: 172.28.0.12 Worker port: 44009 Worker PID: 14097 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","ERROR:flwr:Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","ERROR:flwr:The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-06-02 19:11:39,453 E 13974 13974] (raylet) local_object_manager.cc:360: Failed to send object spilling request: GrpcUnavailable: RPC Error message: Socket closed; RPC Error details: \n","\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-06-02 19:12:03,481 E 13974 13974] (raylet) node_manager.cc:3084: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n","\u001b[2m\u001b[33m(raylet)\u001b[0m \n","\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-06-02 19:12:31,019 E 13974 13974] (raylet) local_object_manager.cc:360: Failed to send object spilling request: GrpcUnavailable: RPC Error message: Socket closed; RPC Error details: \n","\u001b[93mWARNING \u001b[0m:   Actor(38213e3b44769b9bfdeabb9c01000000) will be remove from pool.\n","WARNING:flwr:Actor(38213e3b44769b9bfdeabb9c01000000) will be remove from pool.\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 289, in _fetch_future_result\n","    raise ex\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 289, in _fetch_future_result\n","    raise ex\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\n","\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\n","\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","ERROR:flwr:The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","ERROR:flwr:Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","ERROR:flwr:Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","ERROR:flwr:The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\u001b[93mWARNING \u001b[0m:   Actor(38213e3b44769b9bfdeabb9c01000000) will be remove from pool.\n","ERROR:flwr:Task was killed due to the node running low on memory.\n","Memory on the node (IP: 172.28.0.12, ID: 3d6a9c6d27d5e3804ccbac617b7fcd04d599c1e1e36dc27800c43614) where the task (actor ID: f905398f62c70378eaba07d701000000, name=ClientAppActor.__init__, pid=14098, memory used=0.89GB) was running was 12.04GB / 12.67GB (0.950265), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-136ec8bc7c10c20e3fbcebe100224af52e231d851d99a78fbfa03200*out -ip 172.28.0.12. Top 10 memory users:\n","PID\tMEM(GB)\tCOMMAND\n","13469\t4.40\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c2f27d49-6ea0...\n","14889\t1.44\tray::SPILL_SpillWorker\n","14867\t1.43\tray::SPILL_SpillWorker\n","14098\t0.89\tray::ClientAppActor.run\n","14097\t0.87\tray::ClientAppActor.run\n","13746\t0.28\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:661a0e1879631aa5d...\n","106\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n","14012\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n","13946\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n","13945\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n","Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n","WARNING:flwr:Actor(38213e3b44769b9bfdeabb9c01000000) will be remove from pool.\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 289, in _fetch_future_result\n","    raise ex\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n","    out_mssg, updated_context = self.actor_pool.get_client_result(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n","    return self._fetch_future_result(cid)\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 289, in _fetch_future_result\n","    raise ex\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n","    res_cid, out_mssg, updated_context = ray.get(\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\n","\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","ERROR:flwr:The actor died unexpectedly before finishing this task.\n","\tclass_name: ClientAppActor\n","\tactor_id: 38213e3b44769b9bfdeabb9c01000000\n","\tpid: 14097\n","\tnamespace: 8859a1c0-bd4c-4d46-bfb6-30903aaa2672\n","\tip: 172.28.0.12\n","The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n","\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 6 failures\n","INFO:flwr:aggregate_fit: received 2 results and 6 failures\n","\u001b[91mERROR \u001b[0m:     name 'aggregate' is not defined\n","ERROR:flwr:name 'aggregate' is not defined\n","\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\", line 308, in start_simulation\n","    hist = run_fl(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 483, in run_fl\n","    hist, elapsed_time = server.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 113, in fit\n","    res_fit = self.fit_round(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 249, in fit_round\n","    ] = self.strategy.aggregate_fit(server_round, results, failures)\n","  File \"\u003cipython-input-5-f88f1036543a\u003e\", line 98, in aggregate_fit\n","    aggregate(weights_results)\n","NameError: name 'aggregate' is not defined\n","\n","ERROR:flwr:Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\", line 308, in start_simulation\n","    hist = run_fl(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 483, in run_fl\n","    hist, elapsed_time = server.fit(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 113, in fit\n","    res_fit = self.fit_round(\n","  File \"/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\", line 249, in fit_round\n","    ] = self.strategy.aggregate_fit(server_round, results, failures)\n","  File \"\u003cipython-input-5-f88f1036543a\u003e\", line 98, in aggregate_fit\n","    aggregate(weights_results)\n","NameError: name 'aggregate' is not defined\n","\n","\u001b[91mERROR \u001b[0m:     Your simulation crashed :(. This could be because of several reasons. The most common are: \n","\t \u003e Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: \n","\t\t - You might be using a class attribute in your clients that hasn't been defined.\n","\t\t - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).\n","\t\t - The return types of methods in your clients/strategies might be incorrect.\n","\t \u003e Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n","\t \u003e All the actors in your pool crashed. This could be because: \n","\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. \n","\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 0.0}.\n","Take a look at the Flower simulation examples for guidance \u003chttps://flower.ai/docs/framework/how-to-run-simulations.html\u003e.\n","ERROR:flwr:Your simulation crashed :(. This could be because of several reasons. The most common are: \n","\t \u003e Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: \n","\t\t - You might be using a class attribute in your clients that hasn't been defined.\n","\t\t - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).\n","\t\t - The return types of methods in your clients/strategies might be incorrect.\n","\t \u003e Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n","\t \u003e All the actors in your pool crashed. This could be because: \n","\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. \n","\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 0.0}.\n","Take a look at the Flower simulation examples for guidance \u003chttps://flower.ai/docs/framework/how-to-run-simulations.html\u003e.\n"]},{"ename":"RuntimeError","evalue":"Simulation crashed.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 308\u001b[0;31m         hist = run_fl(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mrun_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;34m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 483\u001b[0;31m     hist, elapsed_time = server.fit(\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# Train model and replace previous global model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 113\u001b[0;31m             res_fit = self.fit_round(\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mserver_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mfit_round\u001b[0;34m(self, server_round, timeout)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScalar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 249\u001b[0;31m         ] = self.strategy.aggregate_fit(server_round, results, failures)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-5-f88f1036543a\u003e\u001b[0m in \u001b[0;36maggregate_fit\u001b[0;34m(self, server_round, results, failures)\u001b[0m\n\u001b[1;32m     97\u001b[0m         parameters_aggregated = ndarrays_to_parameters(\n\u001b[0;32m---\u003e 98\u001b[0;31m             \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         )\n","\u001b[0;31mNameError\u001b[0m: name 'aggregate' is not defined","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-9-bd83e0ad86dc\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 47\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mclient_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleClientManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Start simulation (training FL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 47\u001b[0;31m commun_metrics_history = fl.simulation.start_simulation(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mclient_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_nodes_glob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mclient_resources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         )\n\u001b[0;32m--\u003e 344\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Simulation crashed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Simulation crashed."]}],"source":["from flwr.common import (\n","    FitRes,\n","    MetricsAggregationFn,\n","    NDArrays,\n","    Parameters,\n","    Scalar,\n","    ndarrays_to_parameters,\n","    parameters_to_ndarrays,\n",")\n","# Define number of epochs per local node\n","epochs = 1\n","\n","# Define number of communication rounds\n","comms_round = 300\n","seed_value = 42\n","tf.random.set_seed(seed_value)\n","initial_model = get_model()\n","\n","# Define dunction to pass to each local node (client)\n","def client_fn(cid: str) -\u003e fl.client.Client:\n","    # Define model\n","    model = get_model()\n","\n","    # Load data partition of each client ID (cid)\n","    x_train_cid = np.array(list_x_train[int(cid)],dtype=float)\n","    y_train_cid = np.array(list_y_train[int(cid)],dtype=float)\n","\n","    # Define test data (taken from the centralized data to compare performance of CL and FL)\n","    x_test_cid = np.array(test_images)\n","    y_test_cid = np.array(test_labels)\n","\n","    # Create and return client\n","    return FlowerClient(model, x_train_cid, y_train_cid, x_test_cid, y_test_cid, epochs)\n","\n","# Create Federated strategy\n","strategy= FedWeightsLoss(\n","        fraction_fit=0.2,  # Sample 100% of available clients for training\n","        fraction_evaluate=0.0,  # Sample 50% of available clients for evaluation\n","        min_fit_clients=8,\n","        min_evaluate_clients = 0,\n","        min_available_clients = 40,\n","        evaluate_fn=evaluate_DNN_CL,\n","        initial_parameters  = ndarrays_to_parameters(initial_model.get_weights())\n",")\n","client_manager = SimpleClientManager()\n","# Start simulation (training FL)\n","commun_metrics_history = fl.simulation.start_simulation(\n","    client_fn=client_fn,\n","    num_clients=local_nodes_glob,\n","    config=fl.server.ServerConfig(num_rounds=comms_round),\n","    strategy=strategy,\n","    client_manager = client_manager\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"abOmWtuVSGXd"},"outputs":[],"source":["# Define metrics to plot\n","metrics_show = [\"accuracy\",\"precision\",\"recall\",\"f1score\"]\n","\n","# Define dimensions for plot\n","f, axs = plt.subplots(1,len(metrics_show),figsize=(70,15))\n","\n","# Loop over the communication round history and metrics\n","for i in range(len(metrics_show)):\n","  plt.subplot(1, len(metrics_show), i + 1)\n","  plot_metric_from_history(commun_metrics_history,\"any\",\"centralized\",metrics_show[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4efk-UG_l6iy"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","provenance":[{"file_id":"1GkYDkGoVLqrDWnBaiMkQcL0nqFOOlqIk","timestamp":1708008277063},{"file_id":"1dXOyMEaevdSfmHMlwoBFeguGXCIBt7He","timestamp":1708005294544},{"file_id":"1l7py4SxmijkssCPMUjHoEkHCdSoREADo","timestamp":1707997490978},{"file_id":"1glCSwmF7BJ4mmkgkLrcpo28ZxVYhKd1w","timestamp":1707992856785},{"file_id":"1b7lOEPkK9wzVhCNZYYZoDVLa98FtNHDq","timestamp":1707992811065},{"file_id":"1mbiMNSPgOAg-mRHKeI-ZKZHvpXUTeb9U","timestamp":1707992750140},{"file_id":"1ATpSnkl7F4noWf_uComiTmJtKjxITlRT","timestamp":1707992133909},{"file_id":"1JulnbxCR5T6hVbPCHVZGL8lw01YKARO8","timestamp":1707990259381},{"file_id":"1XEMccQFJqkG3U04qw3g-Be8Nnu6TzCEt","timestamp":1707987786338}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
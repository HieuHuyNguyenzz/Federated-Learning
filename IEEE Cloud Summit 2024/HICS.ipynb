{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAp6y2kHIsqa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import time\n",
        "from collections import Counter, OrderedDict\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "from fedartml import InteractivePlots, SplitAsFederatedData\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import flwr as fl\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from flwr.common import Metrics\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "import threading\n",
        "from abc import ABC, abstractmethod\n",
        "from logging import INFO\n",
        "from flwr.common.logger import log\n",
        "import random\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.criterion import Criterion\n",
        "from logging import WARNING\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "from functools import reduce\n",
        "\n",
        "from flwr.common import (\n",
        "    FitRes,\n",
        "    FitIns,\n",
        "    MetricsAggregationFn,\n",
        "    NDArrays,\n",
        "    Parameters,\n",
        "    Scalar,\n",
        "    ndarrays_to_parameters,\n",
        "    parameters_to_ndarrays,\n",
        ")\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.server.strategy.fedavg import FedAvg\n",
        "\n",
        "from sklearn.cluster import OPTICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfptiIv3Isqd"
      },
      "outputs": [],
      "source": [
        "def test_model(model, X_test, Y_test):\n",
        "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
        "    logits = model.predict(X_test, batch_size=64, verbose=3, callbacks=[GarbageCollectorCallback()])\n",
        "    y_pred = tf.argmax(logits, axis=1)\n",
        "    loss = cce(Y_test, logits).numpy()\n",
        "    acc = accuracy_score(y_pred, Y_test)\n",
        "    pre = precision_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    rec = recall_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    f1s = f1_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    return loss, acc, pre, rec, f1s\n",
        "\n",
        "def from_FedArtML_to_Flower_format(clients_dict):\n",
        "  list_x_train = []\n",
        "  list_y_train = []\n",
        "  client_names = list(clients_dict.keys())\n",
        "  for client in client_names:\n",
        "    each_client_train=np.array(clients_dict[client],dtype=object)\n",
        "    feat=[]\n",
        "    x_tra=np.array(each_client_train[:, 0])\n",
        "    for row in x_tra:\n",
        "      feat.append(row)\n",
        "    feat=np.array(feat)\n",
        "    y_tra=np.array(each_client_train[:, 1])\n",
        "    list_x_train.append(feat)\n",
        "    list_y_train.append(y_tra)\n",
        "\n",
        "  return list_x_train, list_y_train\n",
        "\n",
        "def get_model():\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='relu', input_shape=(32,32,3), padding='same'), #C1\n",
        "        tf.keras.layers.AveragePooling2D(), #S1\n",
        "        tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='valid'), #C2\n",
        "        tf.keras.layers.AveragePooling2D(), #S2\n",
        "        tf.keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='relu', padding='valid'), #C3\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(84, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=SGD(learning_rate = 0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, x_train, y_train, entropy) -> None:\n",
        "        self.model = model\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.entropy = entropy\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.fit(self.x_train, self.y_train, epochs=1,verbose=3, batch_size = 64, callbacks=[GarbageCollectorCallback()])\n",
        "        return self.model.get_weights(), len(self.x_train), {'entropy': self.entropy}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        return loss, len(self.x_test), {\"accuracy\": acc}\n",
        "\n",
        "def plot_metric_from_history(\n",
        "    hist: None,\n",
        "    save_plot_path: None,\n",
        "    metric_type: None,\n",
        "    metric: None,\n",
        ") -> None:\n",
        "\n",
        "    metric_dict = (\n",
        "        hist.metrics_centralized\n",
        "        if metric_type == \"centralized\"\n",
        "        else hist.metrics_distributed\n",
        "    )\n",
        "    rounds, values = zip(*metric_dict[metric])\n",
        "    plt.plot(np.asarray(rounds), np.asarray(values), color=colors[5], linewidth=5, label='Test')\n",
        "    plt.legend(fontsize=45)\n",
        "    plt.xlabel('Communication round', fontsize=40)\n",
        "    plt.ylabel(metric, fontsize=50)\n",
        "    plt.title(metric, fontsize=60)\n",
        "    plt.xticks(fontsize=30)\n",
        "    plt.yticks(fontsize=30)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "def retrieve_global_metrics(\n",
        "    hist: None,\n",
        "    metric_type: None,\n",
        "    metric: None,\n",
        "    best_metric: None,\n",
        ") -> None:\n",
        "\n",
        "    metric_dict = (\n",
        "        hist.metrics_centralized\n",
        "        if metric_type == \"centralized\"\n",
        "        else hist.metrics_distributed\n",
        "    )\n",
        "    rounds, values = zip(*metric_dict[metric])\n",
        "    if best_metric:\n",
        "      metric_return = max(values)\n",
        "    else:\n",
        "      metric_return = values[-1]\n",
        "    return metric_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC0tQlsfIsqe"
      },
      "outputs": [],
      "source": [
        "class SimpleClientManager(ClientManager):\n",
        "    def __init__(self, cluster_labels, entropies) -> None:\n",
        "        self.clients: Dict[str, ClientProxy] = {}\n",
        "        self._cv = threading.Condition()\n",
        "        self.seed = 0\n",
        "        self.cluster_labels = cluster_labels\n",
        "        self.num_cluster = len(cluster_labels)\n",
        "        self.entropies = entropies\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.clients)\n",
        "\n",
        "    def num_available(self) -> int:\n",
        "        return len(self)\n",
        "\n",
        "    def wait_for(self, num_clients: int, timeout: int = 86400) -> bool:\n",
        "        with self._cv:\n",
        "            return self._cv.wait_for(\n",
        "                lambda: len(self.clients) >= num_clients, timeout=timeout\n",
        "            )\n",
        "\n",
        "    def register(self, client: ClientProxy) -> bool:\n",
        "        if client.cid in self.clients:\n",
        "            return False\n",
        "\n",
        "        self.clients[client.cid] = client\n",
        "        with self._cv:\n",
        "            self._cv.notify_all()\n",
        "        return True\n",
        "\n",
        "    def unregister(self, client: ClientProxy) -> None:\n",
        "        if client.cid in self.clients:\n",
        "            del self.clients[client.cid]\n",
        "            with self._cv:\n",
        "                self._cv.notify_all()\n",
        "\n",
        "    def all(self) -> Dict[str, ClientProxy]:\n",
        "        return self.clients\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        num_clients: int,\n",
        "        min_num_clients: Optional[int] = None,\n",
        "        criterion: Optional[Criterion] = None,\n",
        "    ) -> List[ClientProxy]:\n",
        "        if min_num_clients is None:\n",
        "            min_num_clients = num_clients\n",
        "        self.wait_for(min_num_clients)\n",
        "\n",
        "        available_cids = list(self.clients)\n",
        "        if criterion is not None:\n",
        "            available_cids = [\n",
        "                cid for cid in available_cids if criterion.select(self.clients[cid])\n",
        "            ]\n",
        "        sampled_cids = []\n",
        "\n",
        "        if num_clients == 1:\n",
        "            sampled_cids = random.sample(available_cids, num_clients)\n",
        "\n",
        "        else:\n",
        "            sample_choices = []\n",
        "            sum_entropy = []\n",
        "            for i in range(5):\n",
        "                ss = []\n",
        "                sample_cluster = random.sample(self.cluster_labels, num_clients)\n",
        "                for cluster in sample_cluster:\n",
        "\n",
        "                    client = random.choice(cluster)\n",
        "                    ss.append(client)\n",
        "                sum_ent = sum(np.exp(self.entropies[cid]) for cid in ss)\n",
        "                sum_entropy.append(sum_ent)\n",
        "                sample_choices.append(ss)\n",
        "            sampled_cids = sample_choices[np.argmax(sum_entropy)]\n",
        "            sampled_cids = [str(cid) for cid in sampled_cids]\n",
        "        return [self.clients[cid] for cid in sampled_cids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6SWK5UIsqf"
      },
      "outputs": [],
      "source": [
        "def aggregate(results: List[Tuple[NDArrays, float]]) -> NDArrays:\n",
        "    num_examples_total = sum(num_examples for (_, num_examples) in results)\n",
        "    weighted_weights = [\n",
        "        [layer * num_examples for layer in weights] for weights, num_examples in results\n",
        "    ]\n",
        "    weights_prime: NDArrays = [\n",
        "        reduce(np.add, layer_updates) / num_examples_total\n",
        "        for layer_updates in zip(*weighted_weights)\n",
        "    ]\n",
        "    return weights_prime\n",
        "\n",
        "class FedImp(FedAvg):\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: List[Tuple[ClientProxy, FitRes]],\n",
        "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        if not results:\n",
        "            return None, {}\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "        weights_results = [\n",
        "                (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples*np.exp(fit_res.metrics[\"entropy\"]/0.7))\n",
        "                for _, fit_res in results\n",
        "            ]\n",
        "        aggregated_ndarrays = aggregate(weights_results)\n",
        "\n",
        "        parameters_aggregated = ndarrays_to_parameters(aggregated_ndarrays)\n",
        "        metrics_aggregated = {}\n",
        "        if self.fit_metrics_aggregation_fn:\n",
        "            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n",
        "            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n",
        "        elif server_round == 1:\n",
        "            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n",
        "\n",
        "        return parameters_aggregated, metrics_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSwcTKIFIsqg"
      },
      "outputs": [],
      "source": [
        "random_state = 0\n",
        "colors = [\"#00cfcc\",\"#e6013b\",\"#007f88\",\"#00cccd\",\"#69e0da\",\"darkblue\",\"#FFFFFF\"]\n",
        "local_nodes_glob = 50\n",
        "Alpha = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7IXOlZhIsqg"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255\n",
        "train_labels, test_labels = np.concatenate(train_labels), np.concatenate(test_labels)\n",
        "\n",
        "my_federater = SplitAsFederatedData(random_state = random_state)\n",
        "\n",
        "clients_glob_dic, list_ids_sampled_dic, miss_class_per_node, distances = my_federater.create_clients(image_list = train_images, label_list = train_labels,\n",
        "                                                             num_clients = local_nodes_glob, prefix_cli='client', method = \"dirichlet\", alpha = Alpha)\n",
        "\n",
        "clients_glob = clients_glob_dic['with_class_completion']\n",
        "list_ids_sampled = list_ids_sampled_dic['with_class_completion']\n",
        "\n",
        "list_x_train, list_y_train = from_FedArtML_to_Flower_format(clients_dict=clients_glob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu2IvtKNIsqg",
        "outputId": "c628b65d-1940-4c8a-8501-b3b0e85eac73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8140706749336993, 0.7814997270035137, 0.8007472529181611, 0.8869961625657947, 0.7223111043410904, 0.9456407216451947, 0.8371830227020712, 0.6400505782434818, 0.8553229668681934, 0.8474513248299018, 0.8562247907372218, 0.8689586586794313, 0.6883632928438981, 0.8735449107572161, 0.8693551902686224, 0.6109504488631721, 0.7927503114997031, 0.7350948658039949, 0.7047959241521686, 0.8127940558408161, 0.9088224052763686, 0.8523460377551788, 0.8718265328486052, 0.8099585578440226, 0.8238575186118464, 0.8938044653703818, 0.760999301480173, 0.6635587676972069, 0.7941163902013841, 0.755376368485666, 0.8498132703507625, 0.7354360457254969, 0.9231674957066108, 0.8467628816855375, 0.7859911355340558, 0.8541979540120902, 0.9562638223990053, 0.7284278698166272, 0.707804706099066, 0.8053255114202941, 0.735337802090503, 0.785640571122668, 0.8170941481445381, 0.861795218164332, 0.8474287977345513, 0.7866107201447122, 0.878673082139249, 0.8383190241982307, 0.7932174109028591, 0.7041119561001181]\n"
          ]
        }
      ],
      "source": [
        "def calculate_entropy(y_train):\n",
        "      counts = Counter(y_train)\n",
        "      entropy = 0.0\n",
        "      counts = list(counts.values())\n",
        "      counts = [0 if value is None else value for value in counts]\n",
        "      for value in counts:\n",
        "          entropy += -value/sum(counts) * math.log(value/sum(counts), 10) if value != 0 else 0\n",
        "      return entropy\n",
        "\n",
        "entropies = [calculate_entropy(np.array(list_y_train[int(cid)],dtype=int)) for cid in range(len(list_y_train))]\n",
        "print(entropies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAXOohR-Isqh",
        "outputId": "6fb45a90-6844-40c1-d455-1a7c15eec62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([[5, 36], [8, 13], [9, 48], [16, 21], [22, 23], [25, 35], [45, 49], [10, 33, 34], [14, 43, 46], [29, 31, 39, 40], [0], [1], [2], [3], [4], [6], [7], [11], [12], [15], [17], [18], [19], [20], [24], [26], [27], [28], [30], [32], [37], [38], [41], [42], [44], [47]])\n",
            "[[36, 5], [13, 8], [9, 48], [21, 16], [22, 23], [25, 35], [45, 49], [10, 33, 34], [46, 14, 43], [39, 29, 31, 40], [0], [1], [2], [3], [4], [6], [7], [11], [12], [15], [17], [18], [19], [20], [24], [26], [27], [28], [30], [32], [37], [38], [41], [42], [44], [47]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "def split_clusters(labels):\n",
        "    clusters = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        if label not in clusters:\n",
        "            clusters[label] = [i]\n",
        "        else:\n",
        "            clusters[label].append(i)\n",
        "    return clusters\n",
        "\n",
        "def hellinger_distance(p, q):\n",
        "    return np.sqrt(0.5 * ((np.sqrt(p) - np.sqrt(q)) ** 2).sum())\n",
        "\n",
        "def compute_hellinger_distance_matrix(distributions):\n",
        "    n = len(distributions)\n",
        "    distances = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            distances[i, j] = hellinger_distance(distributions[i], distributions[j])\n",
        "            distances[j, i] = distances[i, j]\n",
        "    return distances\n",
        "\n",
        "def to_prob_dist(data):\n",
        "    return data / np.sum(data, axis=1, keepdims=True)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    sum_exp_x = np.sum(exp_x)\n",
        "    return exp_x / sum_exp_x\n",
        "\n",
        "def kmeans_no_small_clusters(data):\n",
        "    counts = [dict(sorted(Counter(d).items())) for d in list_y_train]\n",
        "\n",
        "    counts = [list(c.values()) for c in counts]\n",
        "    counts = [0 if value is None else value for value in counts]\n",
        "    counts = to_prob_dist(counts)\n",
        "    distance_matrix = compute_hellinger_distance_matrix(counts)\n",
        "\n",
        "    clustering = OPTICS(min_samples=2,\n",
        "                  metric=\"precomputed\").fit(distance_matrix)\n",
        "    labels = split_clusters(clustering.labels_)\n",
        "    labels = dict(sorted(labels.items(), key=lambda item: len(item[1])))\n",
        "    new_dict = labels.copy()\n",
        "    del new_dict[-1]\n",
        "\n",
        "    new_key = max(new_dict.keys()) + 1\n",
        "    for index, value in enumerate(labels[-1]):\n",
        "        while new_key in new_dict:\n",
        "            new_key += 1\n",
        "        new_dict[new_key] = [value]\n",
        "        new_key += 1\n",
        "    return new_dict\n",
        "\n",
        "labels = kmeans_no_small_clusters(list_y_train).values()\n",
        "\n",
        "print(labels)\n",
        "\n",
        "def sortf(item):\n",
        "    return entropies[item]\n",
        "\n",
        "new_label = []\n",
        "for label in labels:\n",
        "    label = sorted(label, key = sortf, reverse=True)\n",
        "    new_label.append(label)\n",
        "\n",
        "print(new_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7BuMIeIsqh"
      },
      "outputs": [],
      "source": [
        "def evaluate_DNN_CL(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    net = get_model()\n",
        "    net.set_weights(parameters)\n",
        "    loss, accuracy, precision, recall, f1score  = test_model(net, test_images, test_labels)\n",
        "    print(f\"@@@@@@ Server-side evaluation loss {loss} / accuracy {accuracy} / f1score {f1score} @@@@@@\")\n",
        "    return loss, {\"accuracy\": accuracy,\"precision\": precision,\"recall\": recall,\"f1score\": f1score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjASQS8rIsqi"
      },
      "outputs": [],
      "source": [
        "comms_round = 1000\n",
        "\n",
        "def client_fn(cid: str) -> fl.client.Client:\n",
        "    # Define model\n",
        "    model = get_model()\n",
        "\n",
        "    x_train_cid = np.array(list_x_train[int(cid)],dtype=float)\n",
        "    y_train_cid = np.array(list_y_train[int(cid)],dtype=int)\n",
        "    return FlowerClient(model, x_train_cid, y_train_cid, entropies[int(cid)])\n",
        "\n",
        "strategy=FedImp(\n",
        "        fraction_fit=0.2,  \n",
        "        fraction_evaluate=0,  \n",
        "        min_fit_clients=10,\n",
        "        min_available_clients = 50,\n",
        "        evaluate_fn=evaluate_DNN_CL,\n",
        ")\n",
        "\n",
        "clientmanager = SimpleClientManager(cluster_labels = new_label, entropies=entropies)\n",
        "\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=local_nodes_glob,\n",
        "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
        "    strategy=strategy,\n",
        "    client_manager = clientmanager,\n",
        "    client_resources = {'num_cpus': 1, 'num_gpus': 0},\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "commun_metrics_history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=local_nodes_glob,\n",
        "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
        "    strategy=strategy,\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "global_acc_test = retrieve_global_metrics(commun_metrics_history,\"centralized\",\"accuracy\",False)\n",
        "\n",
        "global_pre_test = retrieve_global_metrics(commun_metrics_history,\"centralized\",\"precision\",False)\n",
        "\n",
        "global_rec_test = retrieve_global_metrics(commun_metrics_history,\"centralized\",\"recall\",False)\n",
        "\n",
        "global_f1s_test = retrieve_global_metrics(commun_metrics_history,\"centralized\",\"f1score\",False)\n",
        "\n",
        "print(\"\\n\\nFINAL RESULTS: ===========================================================================================================================================================================================\")\n",
        "print('Test: commun_round: {} | global_acc: {:} | global_pre: {} | global_rec: {} | global_f1s: {}'.format(comms_round, global_acc_test, global_pre_test, global_rec_test, global_f1s_test))\n",
        "print(\"Training time: %s seconds\" % (training_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQuuvrQ3Isqj"
      },
      "outputs": [],
      "source": [
        "metrics_show = [\"accuracy\",\"precision\",\"recall\",\"f1score\"]\n",
        "\n",
        "# Define dimensions for plot\n",
        "f, axs = plt.subplots(1,len(metrics_show),figsize=(70,15))\n",
        "\n",
        "# Loop over the communication round history and metrics\n",
        "for i in range(len(metrics_show)):\n",
        "  plt.subplot(1, len(metrics_show), i + 1)\n",
        "  plot_metric_from_history(commun_metrics_history,\"any\",\"centralized\",metrics_show[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAp6y2kHIsqa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gc\n",
        "\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "from fedartml import InteractivePlots, SplitAsFederatedData\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import flwr as fl\n",
        "\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "from flwr.common import Metrics\n",
        "import random\n",
        "from keras.datasets import cifar10, mnist\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    MetricsAggregationFn,\n",
        "    NDArrays,\n",
        "    Parameters,\n",
        "    Scalar,\n",
        "    ndarrays_to_parameters,\n",
        "    parameters_to_ndarrays,\n",
        ")\n",
        "from flwr.common.logger import log\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.criterion import Criterion\n",
        "from flwr.server.strategy.aggregate import aggregate\n",
        "from flwr.server.strategy.fedavg import FedAvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfptiIv3Isqd",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def test_model(model, X_test, Y_test):\n",
        "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
        "    logits = model.predict(X_test, batch_size=64, verbose=3, callbacks=[GarbageCollectorCallback()])\n",
        "    y_pred = tf.argmax(logits, axis=1)\n",
        "    loss = cce(Y_test, logits).numpy()\n",
        "    acc = accuracy_score(y_pred, Y_test)\n",
        "    pre = precision_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    rec = recall_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    f1s = f1_score(y_pred, Y_test, average='weighted',zero_division = 0)\n",
        "    return loss, acc, pre, rec, f1s\n",
        "\n",
        "def from_FedArtML_to_Flower_format(clients_dict):\n",
        "  list_x_train = []\n",
        "  list_y_train = []\n",
        "  client_names = list(clients_dict.keys())\n",
        "  for client in client_names:\n",
        "    each_client_train=np.array(clients_dict[client],dtype=object)\n",
        "    feat=[]\n",
        "    x_tra=np.array(each_client_train[:, 0])\n",
        "    for row in x_tra:\n",
        "      feat.append(row)\n",
        "    feat=np.array(feat)\n",
        "    y_tra=np.array(each_client_train[:, 1])\n",
        "    list_x_train.append(feat)\n",
        "    list_y_train.append(y_tra)\n",
        "\n",
        "  return list_x_train, list_y_train\n",
        "\n",
        "def get_model():\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='relu', input_shape=(32,32,3), padding='same'), #C1\n",
        "        tf.keras.layers.AveragePooling2D(), #S1\n",
        "        tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='valid'), #C2\n",
        "        tf.keras.layers.AveragePooling2D(), #S2\n",
        "        tf.keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='relu', padding='valid'), #C3\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(84, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=SGD(learning_rate = 0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, x_train, y_train, cid) -> None:\n",
        "        self.model = model\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.cid = int(cid)\n",
        "        entropies[self.cid]*=0.995\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "    \n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.compile(optimizer=SGD(learning_rate = config[\"learning_rate\"]), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        self.model.fit(self.x_train, self.y_train, epochs=1,verbose=3, batch_size = 64, callbacks=[GarbageCollectorCallback()])\n",
        "        return self.model.get_weights(), len(self.x_train), {\"entropy\": entropies[self.cid]}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        return loss, len(self.x_test), {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSwcTKIFIsqg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "random_state = 0\n",
        "colors = [\"#00cfcc\",\"#e6013b\",\"#007f88\",\"#00cccd\",\"#69e0da\",\"darkblue\",\"#FFFFFF\"]\n",
        "local_nodes_glob = 40\n",
        "Alpha1 = 100\n",
        "Alpha2 = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7IXOlZhIsqg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255\n",
        "train_labels, test_labels = np.concatenate(train_labels), np.concatenate(test_labels)\n",
        "\n",
        "train1, train2, test1, test2 = train_test_split(train_images, train_labels, test_size=0.25, random_state=random_state)\n",
        "\n",
        "my_federater = SplitAsFederatedData(random_state = random_state)\n",
        "\n",
        "clients_glob_dic1, list_ids_sampled_dic1, miss_class_per_node1, distances1 = my_federater.create_clients(image_list = train1, label_list = test1,\n",
        "                                                             num_clients = 30, prefix_cli='client', method = \"dirichlet\", alpha = Alpha1)\n",
        "\n",
        "clients_glob1 = clients_glob_dic1['with_class_completion']\n",
        "list_ids_sampled1 = list_ids_sampled_dic1['with_class_completion']\n",
        "\n",
        "clients_glob_dic2, list_ids_sampled_dic2, miss_class_per_node2, distances2 = my_federater.create_clients(image_list = train2, label_list = test2,\n",
        "                                                             num_clients = 10, prefix_cli='client', method = \"dirichlet\", alpha = Alpha2)\n",
        "\n",
        "clients_glob2 = clients_glob_dic2['with_class_completion']\n",
        "list_ids_sampled2 = list_ids_sampled_dic2['with_class_completion']\n",
        "\n",
        "list_x_train1, list_y_train1 = from_FedArtML_to_Flower_format(clients_dict=clients_glob1)\n",
        "list_x_train2, list_y_train2 = from_FedArtML_to_Flower_format(clients_dict=clients_glob2)\n",
        "\n",
        "list_x_train, list_y_train = (list_x_train1+list_x_train2), (list_y_train1+list_y_train2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu2IvtKNIsqg",
        "metadata": {},
        "outputId": "c628b65d-1940-4c8a-8501-b3b0e85eac73"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def calculate_entropy(y_train):\n",
        "      counts = Counter(y_train)\n",
        "      entropy = 0.0\n",
        "      counts = list(counts.values())\n",
        "      counts = [0 if value is None else value for value in counts]\n",
        "      for value in counts:\n",
        "          entropy += -value/sum(counts) * math.log(value/sum(counts), 10) if value != 0 else 0\n",
        "      return entropy\n",
        "\n",
        "entropies = [calculate_entropy(np.array(list_y_train[int(cid)],dtype=int)) for cid in range(len(list_y_train))]\n",
        "tem = (1-(np.std(entropies)+0.01)/(np.mean(entropies)+0.01))\n",
        "print(entropies)\n",
        "print(tem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_srs_wr(elements, weights, sample_size):\n",
        "    assert len(elements) == len(weights), \"Danh sách phần tử và trọng số phải có cùng độ dài.\"\n",
        "    assert sample_size <= len(elements), \"Kích thước mẫu phải nhỏ hơn hoặc bằng số phần tử.\"\n",
        "\n",
        "    elements_copy = elements[:]\n",
        "    weights_copy = [len(elements[i])/(weights[i]*40) for i in range(len(weights))]\n",
        "\n",
        "    sample = []\n",
        "\n",
        "    for _ in range(sample_size):\n",
        "        total_weight = sum(weights_copy)\n",
        "        random_choice = random.uniform(0, total_weight)\n",
        "        cumulative_weight = 0\n",
        "        for i, weight in enumerate(weights_copy):\n",
        "            cumulative_weight += weight\n",
        "            if cumulative_weight >= random_choice:\n",
        "                sample.append(elements_copy[i])\n",
        "                elements_copy.pop(i)\n",
        "                weights_copy.pop(i)\n",
        "                weights[i] += 1\n",
        "                weights[i] = len(elements[i])/(weights[i]*40)\n",
        "                break\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC0tQlsfIsqe",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "class SimpleClientManager(ClientManager):\n",
        "    def __init__(self, cluster_labels, entropies) -> None:\n",
        "        self.clients: Dict[str, ClientProxy] = {}\n",
        "        self._cv = threading.Condition()\n",
        "        self.cluster_labels = cluster_labels\n",
        "        self.num_cluster = len(cluster_labels)\n",
        "        self.entropies = entropies\n",
        "        self.choosen = [1 for _ in range(len(cluster_labels))]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.clients)\n",
        "\n",
        "    def num_available(self) -> int:\n",
        "        return len(self)\n",
        "\n",
        "    def wait_for(self, num_clients: int, timeout: int = 86400) -> bool:\n",
        "        with self._cv:\n",
        "            return self._cv.wait_for(\n",
        "                lambda: len(self.clients) >= num_clients, timeout=timeout\n",
        "            )\n",
        "\n",
        "    def register(self, client: ClientProxy) -> bool:\n",
        "        if client.cid in self.clients:\n",
        "            return False\n",
        "\n",
        "        self.clients[client.cid] = client\n",
        "        with self._cv:\n",
        "            self._cv.notify_all()\n",
        "        return True\n",
        "\n",
        "    def unregister(self, client: ClientProxy) -> None:\n",
        "        if client.cid in self.clients:\n",
        "            del self.clients[client.cid]\n",
        "            with self._cv:\n",
        "                self._cv.notify_all()\n",
        "\n",
        "    def all(self) -> Dict[str, ClientProxy]:\n",
        "        return self.clients\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        num_clients: int,\n",
        "        min_num_clients: Optional[int] = None,\n",
        "        criterion: Optional[Criterion] = None,\n",
        "    ) -> List[ClientProxy]:\n",
        "        if min_num_clients is None:\n",
        "            min_num_clients = num_clients\n",
        "        self.wait_for(min_num_clients)\n",
        "\n",
        "        available_cids = list(self.clients)\n",
        "        if criterion is not None:\n",
        "            available_cids = [\n",
        "                cid for cid in available_cids if criterion.select(self.clients[cid])\n",
        "            ]\n",
        "        sampled_cids = []\n",
        "\n",
        "        if num_clients == 1:\n",
        "            sampled_cids = random.sample(available_cids, num_clients)\n",
        "\n",
        "        else:\n",
        "            sample_choices = []\n",
        "            sum_entropy = []\n",
        "            sample_cluster = weighted_srs_wr(self.cluster_labels, self.choosen, num_clients)\n",
        "            for cluster in sample_cluster:\n",
        "                self.choosen[self.cluster_labels.index(cluster)]+=1\n",
        "            for i in range(1):\n",
        "                ss = []\n",
        "                sum_ent = 0\n",
        "                for cluster in sample_cluster:\n",
        "                    client = random.choice(cluster)\n",
        "                    ss.append(client)\n",
        "                    sum_ent += entropies[client]\n",
        "                sum_entropy.append(sum_ent)\n",
        "                sample_choices.append(ss)\n",
        "            sampled_cids = sample_choices[np.argmax(sum_entropy)]\n",
        "            sampled_cids = [str(cid) for cid in sampled_cids]\n",
        "        return [self.clients[cid] for cid in sampled_cids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6SWK5UIsqf",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class FedImp(FedAvg):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        fraction_fit: float = 1.0,\n",
        "        fraction_evaluate: float = 1.0,\n",
        "        min_fit_clients: int = 2,\n",
        "        min_evaluate_clients: int = 2,\n",
        "        min_available_clients: int = 2,\n",
        "        evaluate_fn: Optional[\n",
        "            Callable[\n",
        "                [int, NDArrays, Dict[str, Scalar]],\n",
        "                Optional[Tuple[float, Dict[str, Scalar]]],\n",
        "            ]\n",
        "        ] = None,\n",
        "        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        accept_failures: bool = True,\n",
        "        initial_parameters: Optional[Parameters] = None,\n",
        "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        tem =0.7\n",
        "    \n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.fraction_fit = fraction_fit\n",
        "        self.fraction_evaluate = fraction_evaluate\n",
        "        self.min_fit_clients = min_fit_clients\n",
        "        self.min_evaluate_clients = min_evaluate_clients\n",
        "        self.min_available_clients = min_available_clients\n",
        "        self.evaluate_fn = evaluate_fn\n",
        "        self.on_fit_config_fn = on_fit_config_fn\n",
        "        self.on_evaluate_config_fn = on_evaluate_config_fn\n",
        "        self.accept_failures = accept_failures\n",
        "        self.initial_parameters = initial_parameters\n",
        "        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n",
        "        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n",
        "        self.learning_rate = 0.1\n",
        "        self.decay = 0.995\n",
        "        self.tem = tem\n",
        "\n",
        "        \n",
        "    def configure_fit(\n",
        "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
        "    ) -> List[Tuple[ClientProxy, FitIns]]:\n",
        "        config = {\"learning_rate\": self.learning_rate} \n",
        "        \n",
        "        if self.on_fit_config_fn is not None:\n",
        "            # Custom fit config function provided\n",
        "            config = self.on_fit_config_fn(server_round)\n",
        "        fit_ins = FitIns(parameters, config)\n",
        "        self.learning_rate*=self.decay\n",
        "        # Sample clients\n",
        "        sample_size, min_num_clients = self.num_fit_clients(\n",
        "            client_manager.num_available()\n",
        "        )\n",
        "        clients = client_manager.sample(\n",
        "            num_clients=sample_size, min_num_clients=min_num_clients\n",
        "        )\n",
        "\n",
        "        return [(client, fit_ins) for client in clients]\n",
        "    \n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: List[Tuple[ClientProxy, FitRes]],\n",
        "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        if not results:\n",
        "            return None, {}\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "        weights_results = [\n",
        "                (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples*np.exp(fit_res.metrics[\"entropy\"]/self.tem))\n",
        "            \n",
        "                for _, fit_res in results\n",
        "            ]\n",
        "\n",
        "        aggregated_ndarrays = aggregate(weights_results)\n",
        "        parameters_aggregated = ndarrays_to_parameters(aggregated_ndarrays)\n",
        "        metrics_aggregated = {}\n",
        "        return parameters_aggregated, metrics_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAXOohR-Isqh",
        "metadata": {},
        "outputId": "6fb45a90-6844-40c1-d455-1a7c15eec62a"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "\n",
        "def split_clusters(labels):\n",
        "    clusters = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        if label not in clusters:\n",
        "            clusters[label] = [i]\n",
        "        else:\n",
        "            clusters[label].append(i)\n",
        "    return clusters\n",
        "\n",
        "def hellinger_distance(p, q):\n",
        "    return np.sqrt(0.5 * ((np.sqrt(p) - np.sqrt(q)) ** 2).sum())\n",
        "\n",
        "def compute_hellinger_distance_matrix(distributions):\n",
        "    n = len(distributions)\n",
        "    distances = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            distances[i, j] = hellinger_distance(distributions[i], distributions[j])\n",
        "            distances[j, i] = distances[i, j]\n",
        "    return distances\n",
        "\n",
        "def to_prob_dist(data):\n",
        "    return data / np.sum(data, axis=1, keepdims=True)\n",
        "\n",
        "def kmeans_no_small_clusters(data):\n",
        "    counts = [dict(sorted(Counter(d).items())) for d in list_y_train]\n",
        "\n",
        "    counts = [list(c.values()) for c in counts]\n",
        "    counts = [0 if value is None else value for value in counts]\n",
        "    counts = to_prob_dist(counts)\n",
        "    distance_matrix = compute_hellinger_distance_matrix(counts)\n",
        "\n",
        "    clustering = OPTICS(min_samples=2,\n",
        "                  metric=\"precomputed\").fit(distance_matrix)\n",
        "    labels = split_clusters(clustering.labels_)\n",
        "    labels = dict(sorted(labels.items(), key=lambda item: len(item[1])))\n",
        "    new_dict = labels.copy()\n",
        "    del new_dict[-1]\n",
        "\n",
        "    new_key = max(new_dict.keys()) + 1\n",
        "    for index, value in enumerate(labels[-1]):\n",
        "        while new_key in new_dict:\n",
        "            new_key += 1\n",
        "        new_dict[new_key] = [value]\n",
        "        new_key += 1\n",
        "    return new_dict\n",
        "\n",
        "labels = kmeans_no_small_clusters(list_y_train).values()\n",
        "\n",
        "print(labels)\n",
        "\n",
        "new_label = []\n",
        "for label in labels:\n",
        "    new_label.append(label)\n",
        "\n",
        "print(new_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7BuMIeIsqh",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def evaluate_DNN_CL(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, fl.common.Scalar],\n",
        ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
        "    net = get_model()\n",
        "    net.set_weights(parameters)\n",
        "    loss, accuracy, precision, recall, f1score  = test_model(net, test_images, test_labels)\n",
        "    return loss, {\"accuracy\": accuracy,\"precision\": precision,\"recall\": recall,\"f1score\": f1score}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjASQS8rIsqi",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "comms_round = 1000\n",
        "\n",
        "def client_fn(cid: str) -> fl.client.Client:\n",
        "    # Define model\n",
        "    model = get_model()\n",
        "\n",
        "    x_train_cid = np.array(list_x_train[int(cid)],dtype=float)\n",
        "    y_train_cid = np.array(list_y_train[int(cid)],dtype=int)\n",
        "    return FlowerClient(model, x_train_cid, y_train_cid, cid)\n",
        "\n",
        "strategy=FedImp(\n",
        "        fraction_fit=0.2,  \n",
        "        fraction_evaluate=0,  \n",
        "        min_fit_clients=8,\n",
        "        min_available_clients = 40,\n",
        "        evaluate_fn=evaluate_DNN_CL,\n",
        ")\n",
        "\n",
        "clientmanager = SimpleClientManager(cluster_labels = new_label, entropies=entropies)\n",
        "\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=local_nodes_glob,\n",
        "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
        "    strategy=strategy,\n",
        "    client_manager = clientmanager,\n",
        "    client_resources = {'num_cpus': 1, 'num_gpus': 0},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(f'HICFL30.txt', 'w') as f:\n",
        "    # Write some content to the file\n",
        "    json.dump(history.metrics_centralized[\"accuracy\"], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(history.metrics_centralized[\"precision\"][-1])\n",
        "print(history.metrics_centralized[\"recall\"][-1])\n",
        "print(history.metrics_centralized[\"f1score\"][-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
